#!/usr/bin/env python3
# -*- coding: utf-8 -*-

__author__ = 'Ã–nder Kartal'

import argparse
import io
import os
import sys

import pandas as pd

from shannonlib.core import divergence


def run_divergence(args):

    if os.path.isfile(args.output) and not os.stat(args.output).st_size == 0:
        msg = "-- Stopped!\n-- Output file exists and is not empty."
        sys.exit(msg)

    meta = args.metadata.read()

    try:
        sample = pd.read_csv(io.StringIO(meta), comment='#', header=0)
        _ = sample['url']
        _ = sample['label']
    except KeyError:
        try:
            sample = pd.read_csv(io.StringIO(
                meta), comment='#', header=0, sep='\t')
            _ = sample['url']
            _ = sample['label']
        except KeyError:
            msg = ('-- Stopped!\n'
                   '-- Could not parse metadata.\n'
                   '-- 1. Ensure that fields are tab- or comma-separated\n'
                   '-- 2. Ensure that columns "url" and "label" are present')
            sys.exit(msg)

    # GPF data columns
    try:
        assert(len(args.dcols) == len(args.dnames))
    except AssertionError:
        msg = ('-- Stopped!\n'
               '-- Length of --dcols and --dnames must match')
        sys.exit(msg)

    dtypes = [float if args.prob else int] * len(args.dcols)
    dcols = [col - 1 for col in args.dcols]
    gpf_data = [list(zip(dcols, args.dnames, dtypes))]

    # if groupby is not None:
    #     subsample_family = sample.groupby(groupby)
    #     for key, subsample in subsample_family:
    #         filename = groupname(by=groupby, name=key, fname=args.output)
    #         divergence(subsample, chrom=args.sequence, data_columns=gpf_data,
    #                    outfile=filename, chunksize=args.chunksize)
    # else:
    print('processing sequence {} ...'.format(args.sequence))
    divergence(sample, chrom=args.sequence, data_columns=gpf_data,
               outfile=args.output, chunksize=args.chunk)

    return None


if __name__ == '__main__':

    parser = argparse.ArgumentParser()
    subparsers = parser.add_subparsers()

    # main parser
    parser.prog = 'shannon'
    parser.description = 'Command-line interface to %(prog)s.'

    # cluster
    parser_cluster = subparsers.add_parser('cluster')

    # segment
    parser_segment = subparsers.add_parser('seg')

    # divergence
    parser_div = subparsers.add_parser(
        'div', formatter_class=argparse.RawTextHelpFormatter)

    parser_div.set_defaults(func=run_divergence)
    parser_div.help = 'JS Divergence for genome position files (GPFs).'
    parser_div.description = parser_div.help
    parser_div_required = parser_div.add_argument_group('required arguments')

    parser_div.add_argument(
        '--prob', action='store_true',
        help='indicate that data are probabilites (default: counts)')

    parser_div.add_argument(
        '--chunk', metavar='SIZE', default=1e4, type=int,
        help=('set size of data to process in-memory (default: %(default)d)\n'
              '- in terms of expected number of genome positions\n'
              '- higher numbers lead to more memory-hungry, faster computations'))

    parser_div_required.add_argument(
        '-m', '--metadata', metavar='FILE', type=argparse.FileType('r'),
        required=True, help=('metadata for GPFs\n'
                             '- comment lines (#) are ignored\n'
                             '- values must be comma- or tab-separated\n'
                             '- first non-comment line must be header\n'
                             '- "url" and "label" columns are required\n'
                             '- if stdin is metadata use "--metadata -"'))

    parser_div_required.add_argument(
        '-o', '--output', metavar='FILE', required=True, help='output filepath')

    parser_div_required.add_argument(
        '-s', '--sequence', metavar='ID', required=True, type=str,
        help='query sequence (chromosome/scaffold) in GPF')

    parser_div_required.add_argument(
        '-c', '--dcols', metavar='COLN', nargs='+', required=True, type=int,
        help='column numbers (1-based) in GPFs that hold the data')

    parser_div_required.add_argument(
        '-n', '--dnames', metavar='NAME', nargs='+', required=True, type=str,
        help='names of data columns following the order in --dcols')

    # parser.add_argument('-g', '--groupby', metavar='STR', nargs='+', type=str,
    #                     help='''
    #                     The factor according to which the selected set is
    #                     partitioned; has to match column names in the input
    #                     metadata. One or more factors can be given, which
    #                     produces a file for each combination of factors.
    #                     ''')

    args = parser.parse_args()
    args.func(args)
